{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is an SVM and how does it work?\n",
        "\n",
        "- Support Vector Machine (SVM) is a maximum-margin classifier. It finds the hyperplane that separates classes with the largest margin (distance to the nearest training points). Only a few points on the edgeâ€”support vectorsâ€”determine the boundary. For linear data it fits a straight hyperplane; for nonlinear data it uses kernels (see Q3). Training typically minimizes hinge loss with a margin penalty."
      ],
      "metadata": {
        "id": "bSc2jkf-9YrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "- Hard margin: assumes perfect linear separability; no misclassifications allowed. Maximizes margin subject to all points being correctly classified. Very sensitive to noise/outliers; can overfit if data isnâ€™t separable.\n",
        "\n",
        "- Soft margin: allows violations using slack variables and a penalty C. Large C â†’ fewer violations (tighter fit); small C â†’ more margin (better generalization, more tolerant to outliers). Used in practice.\n"
      ],
      "metadata": {
        "id": "ucwn9h4P9Yh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "-  The kernel trick computes inner products in a high-dimensional feature space without explicitly mapping features. Replace âŸ¨x, xâ€²âŸ© with k(x, xâ€²) and learn a linear separator in that space â†’ a nonlinear boundary in original space.\n",
        "\n",
        " Example of a kernel: Radial Basis Function (RBF) kernel also known as the Gaussian kernel\n",
        "\n",
        "   k(x,x â€² )= exp(-Î³ âˆ¥ x - x â€² âˆ¥ 2 )\n",
        "\n",
        " - Use when class boundary is curved/complex.\n",
        "\n",
        " - Î³ controls locality: high Î³ â†’ tighter, wiggly decision boundary; low Î³ â†’ smoother, broader influence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zZp-q-Jz9YY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is a NaÃ¯ve Bayes Classifier and why â€œnaÃ¯veâ€?\n",
        "\n",
        "- NaÃ¯ve Bayes applies Bayesâ€™ Theorem to compute\n",
        "ð‘ƒ(ð‘¦ âˆ£ ð‘¥ ) and predicts the class with the highest posterior. It assumes conditional independence of features given the classâ€”thatâ€™s the â€œnaÃ¯veâ€ part. Despite the simplification, itâ€™s fast, robust, and works well for high-dimensional text."
      ],
      "metadata": {
        "id": "OifzFAIi9YOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli NaÃ¯ve Bayes variants.When would you use each one?\n",
        "- Gaussian NB: assumes each feature is normally distributed within a class.\n",
        "\n",
        " Use for continuous features (e.g., sensor measurements, real-valued attributes).\n",
        "\n",
        "- Multinomial NB: models counts/frequencies (word counts).\n",
        "\n",
        " Use for text classification with bag-of-words or TF-IDF counts.\n",
        "\n",
        "- Bernoulli NB: models binary features (present/absent).\n",
        "\n",
        " Use for text with binary indicators (word present?), or other on/off features."
      ],
      "metadata": {
        "id": "x49xWcph9YEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# practical question\n"
      ],
      "metadata": {
        "id": "vt3yF61yBMSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqmwQAzH9P6M",
        "outputId": "cda53593-1d1b-463d-b37c-50bae901db9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n",
            "Number of support vectors per class: [ 3 10  9]\n",
            "Support vectors (first 5 rows):\n",
            " [[5.1 3.8 1.9 0.4]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.  2.9 4.5 1.5]]\n"
          ]
        }
      ],
      "source": [
        "# 6: Write a Python program to: â— Load the Iris dataset â— Train an SVM Classifier with a linear kernel â— Print the model's accuracy and support vectors\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Linear SVM\n",
        "clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Support vectors\n",
        "print(\"Number of support vectors per class:\", clf.n_support_)\n",
        "print(\"Support vectors (first 5 rows):\\n\", clf.support_vectors_[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.Write a Python program to: â— Load the Breast Cancer dataset â— Train a Gaussian NaÃ¯ve Bayes model â— Print its classification report including precision, recall, and F1-score.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GK1fxV2AFLu",
        "outputId": "437ff5ce-0eaf-42ac-bc86-11da70a23a73"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.96      0.87      0.91        53\n",
            "      benign       0.93      0.98      0.95        90\n",
            "\n",
            "    accuracy                           0.94       143\n",
            "   macro avg       0.94      0.92      0.93       143\n",
            "weighted avg       0.94      0.94      0.94       143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8: Write a Python program to: â— Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma. â— Print the best hyperparameters and accuracy\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    SVC(kernel='rbf', random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best = grid.best_estimator_\n",
        "y_pred = best.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(f\"Test accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBN0e24BAFID",
        "outputId": "d24f44ce-694a-4017-b42e-6485f4a54d65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'C': 100, 'gamma': 'scale'}\n",
            "Test accuracy: 0.8222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # 9: Write a Python program to: â— Train a NaÃ¯ve Bayes Classifier on a synthetic text dataset â— Print the model's ROC-AUC score for its predictions.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load text data\n",
        "cats = None  # or choose a subset like ['sci.space','rec.sport.baseball',...]\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=cats, remove=('headers','footers','quotes'))\n",
        "X_text, y = newsgroups.data, newsgroups.target\n",
        "classes = list(range(len(newsgroups.target_names)))\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_text, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Vectorize (word + char can help; here word n-grams as a simple baseline)\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
        "X_train_t = tfidf.fit_transform(X_train)\n",
        "X_test_t  = tfidf.transform(X_test)\n",
        "\n",
        "# Model\n",
        "clf = MultinomialNB(alpha=0.1)\n",
        "clf.fit(X_train_t, y_train)\n",
        "\n",
        "# ROC-AUC (multiclass, macro-avg)\n",
        "probs = clf.predict_proba(X_test_t)\n",
        "y_test_bin = label_binarize(y_test, classes=classes)\n",
        "auc = roc_auc_score(y_test_bin, probs, average='macro', multi_class='ovr')\n",
        "print(f\"Macro ROC-AUC: {auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSslubWVAFFV",
        "outputId": "6215ac7f-bf2b-4f45-d302-1a67b485c7f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Macro ROC-AUC: 0.9682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine youâ€™re working as a data scientist for a company that handles email communications.Your task is to automatically classify emails as Spam or Not Spam.\n",
        "\n",
        "The emails may contain : â— Text with diverse vocabulary â— Potential class imbalance (far more legitimate emails than spam) â— Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "â— Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "â— Choose and justify an appropriate model (SVM vs. NaÃ¯ve Bayes)\n",
        "â— Address class imbalance\n",
        "â— Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "->  Preprocessing\n",
        "\n",
        " - Text normalization: lowercasing, strip HTML, remove URLs/usernames, normalize unicode.\n",
        "\n",
        "- Tokenization & vectorization:\n",
        "\n",
        "  - Start with TF-IDF: combine word n-grams (1-2) + character n-grams (3-5) to capture misspellings and obfuscations (e.g., â€œvi@gr@â€).\n",
        "\n",
        "  - Optional: lemmatization; usually not required with char n-grams.\n",
        "\n",
        "- Handle missing/incomplete data: fill missing bodies/subjects with empty strings; concatenate subject + body.\n",
        "\n",
        "- Feature selection: optional Ï‡Â² to drop uninformative terms; or rely on linear models regularization.\n",
        "\n",
        "- Meta features (optional): number of links, number of recipients, presence of attachments, ratio of uppercase, etc.\n",
        "\n",
        "\n",
        "-> Model choice (SVM vs. NaÃ¯ve Bayes)\n",
        "\n",
        "- Multinomial NB is a strong, fast baseline for text; it works very well with TF-IDF and massive vocabularies.\n",
        "\n",
        "- Linear SVM (e.g., LinearSVC) typically yields higher accuracy/F1 for spam tasks because it optimizes a discriminative margin in high-dimensional sparse space.\n",
        "\n",
        "- Practical plan: start with MultinomialNB for a baseline; then train LinearSVC (or SGDClassifier(loss='hinge' or 'modified_huber')) and compare. If calibrated probabilities are needed (for thresholding or business rules), wrap SVM with CalibratedClassifierCV.\n",
        "\n",
        "-> Address class imbalance\n",
        "- Use stratified splits and class_weight='balanced' for SVM (or tune class weights).\n",
        "\n",
        "- Adjust decision threshold to optimize business metric (maximize F1 or minimize false positives).\n",
        "\n",
        "- Consider downsampling the majority class or SMOTE on dense meta features; for raw sparse text, class weighting + thresholding usually suffices.\n",
        "\n",
        "- Cost-sensitive evaluation: treat false positives (ham flagged as spam) as more costly than false negatives; reflect this via weights/thresholds.\n",
        "\n",
        "-> Evaluation\n",
        "\n",
        "- Primary metrics:\n",
        "\n",
        "  -  Precision (Spam class): minimize false positives (avoid losing real mail).\n",
        "\n",
        "  -  Recall (Spam class): catch spam.\n",
        "\n",
        "  -  F1 (Spam) or FÎ² with Î²<1 to emphasize precision if business prefers.\n",
        "\n",
        "  -  ROC-AUC and PR-AUC (PR-AUC is more informative for imbalanced data).\n",
        "\n",
        "- Validation: Stratified k-fold CV; keep a chronological holdout if data is time-dependent (concept drift).\n",
        "\n",
        "- Calibration: if using thresholds or triage, use probability calibration (Platt scaling/Isotonic via CalibratedClassifierCV).\n",
        "\n",
        "- Error analysis: inspect most-confused examples; look at top weighted features for each class to detect leakage or bias.\n",
        "\n",
        "-> Business impact\n",
        "\n",
        "- Productivity: less manual triage; employees see more real mail, faster response times.\n",
        "\n",
        "- Risk reduction: fewer phishing/spam reaching inbox; protect users and brand reputation.\n",
        "\n",
        "- Deliverability & CSAT: better filtering reduces user complaints; configurable thresholds reduce false positives that could hide important emails.\n",
        "\n",
        "- Operational efficiency: automated models plus explainable features (top tokens/links) make trust & safety reviews faster."
      ],
      "metadata": {
        "id": "pJO_enOrHN2e"
      }
    }
  ]
}